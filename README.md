# Be More Agent ğŸ¤–
**A Customizable, Local AI Agent for Raspberry Pi**

![Python](https://img.shields.io/badge/Python-3.9%2B-blue) ![Platform](https://img.shields.io/badge/Platform-Raspberry%20Pi-red) ![License](https://img.shields.io/badge/License-MIT-green)

This project turns a Raspberry Pi into a fully functional, conversational AI robot. Unlike cloud-based assistants (Alexa, Siri), this agent runs **100% locally** on your device. It listens for a wake word, processes speech, "thinks" using a local Large Language Model (LLM), and speaks back with a low-latency neural voiceâ€”all while displaying reactive face animations.

**It is designed as a blank canvas:** You provide the "Character Pack" (images and sounds) to give it a personality.

**Example**
[![Watch the video](https://img.youtube.com/vi/l5ggH-YhuAw/maxresdefault.jpg)](https://youtu.be/l5ggH-YhuAw)

## âœ¨ Features

* **100% Local Intelligence**: Powered by **Ollama** (LLM) and **Whisper.cpp** (Speech-to-Text). No API fees, no cloud data usage.
* **Open Source Wake Word**: Wakes up to your custom model using **OpenWakeWord** (Offline & Free).
* **Hardware-Aware Audio**: Automatically detects your microphone's sample rate to prevent ALSA errors on Raspberry Pi.
* **Smart Web Search**: Prioritizes news headlines for queries like "Search for news about..." and falls back to general web results.
* **Reactive Faces**: The GUI updates the character's face based on its state (Listening, Thinking, Speaking, Idle).
* **Fast Text-to-Speech**: Uses **Piper TTS** for low-latency, high-quality voice generation on the Pi.
* **Vision Capable**: Can "see" and describe the world using a connected camera and the **Moondream** vision model.

## ğŸ› ï¸ Hardware Requirements

* **Raspberry Pi 5** (Recommended).
* USB Microphone & Speaker
* LCD Display
* Camera module

---

## ğŸ“‚ Project Structure

After running the setup script, your folder will look like this.
*Note: The `piper` folder is downloaded automatically and should **not** be committed to GitHub.*

```text
pi-local-assistant/
â”œâ”€â”€ agent.py                   # The main script
â”œâ”€â”€ setup.sh                   # Auto-installer script
â”œâ”€â”€ wakeword.onnx              # OpenWakeWord model file (User Provided)
â”œâ”€â”€ config.json                # User configuration
â”œâ”€â”€ memory.json                # Auto-generated chat history
â”œâ”€â”€ piper/                     # (Auto-generated by setup.sh - DO NOT COMMIT)
â”‚   â”œâ”€â”€ piper                  # Executable binary
â”‚   â””â”€â”€ en_GB-semaine...onnx   # Default voice model
â”œâ”€â”€ sounds/                    # Sound effects folder (User Provided)
â”‚   â”œâ”€â”€ greeting_sounds/       # Folder for startup .wav files
â”‚   â”œâ”€â”€ thinking_sounds/       # Folder for looping .wav files
â”‚   â”œâ”€â”€ ack_sounds/            # Folder for "I heard you" .wav files
â”‚   â””â”€â”€ error_sounds/          # Folder for "Unknown command" .wav files
â””â”€â”€ faces/                     # Face images folder (User Provided)
    â”œâ”€â”€ idle/                  # .png sequence for idle state
    â”œâ”€â”€ listening/             # .png sequence for listening
    â”œâ”€â”€ thinking/              # .png sequence for thinking
    â”œâ”€â”€ speaking/              # .png sequence for speaking
    â”œâ”€â”€ error/                 # .png sequence for errors
    â””â”€â”€ warmup/                # .png sequence for startup
```

---

## ğŸš€ Installation

### Option 1: Quick Install (Recommended)
1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/brenpoly/be-more-agent.git](https://github.com/brenpoly/be-more-agent.git)
    cd be-more-agent
    ```

2.  **Create & Activate Virtual Environment (Required):**
    *You must do this to install Python libraries on Raspberry Pi.*
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Run the setup script:**
    This script downloads the voice models and installs all dependencies into your virtual environment.
    ```bash
    chmod +x setup.sh
    ./setup.sh
    ```

4.  **Run the Agent:**
    ```bash
    python agent.py
    ```

### Option 2: Manual Install
If you prefer to set it up yourself:

1.  **Install System Deps:**
    ```bash
    sudo apt install libasound2-dev portaudio19-dev libportaudio2
    ```

2.  **Create & Activate Virtual Environment:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Install Python Deps:**
    ```bash
    pip install sounddevice numpy openwakeword ollama ddgs scipy pillow
    ```

4.  **Get Piper & Voice Models:**
    Download the [Piper aarch64 binary](https://github.com/rhasspy/piper/releases) and the [en_GB-semaine-medium.onnx](https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_GB/semaine/medium/en_GB-semaine-medium.onnx) voice model. Extract them into a folder named `piper/` in the root directory.

### Option 2: Manual Install
If you prefer to set it up yourself (or aren't on a Raspberry Pi):

1.  **Install System Deps:** `sudo apt install libasound2-dev portaudio19-dev libportaudio2`
2.  **Install Python Deps:** `pip install sounddevice numpy openwakeword ollama ddgs scipy pillow`
3.  **Get Piper:** Download the [Piper aarch64 binary](https://github.com/rhasspy/piper/releases) and a [voice model](https://huggingface.co/rhasspy/piper-voices), extract them into a folder named `piper/`.

---

## ğŸ“‚ Configuration (`config.json`)

You can modify the hardware behavior and personality in `config.json`.

```json
{
    "text_model": "gemma3:1b",
    "vision_model": "moondream",
    "voice_model": "piper/en_GB-semaine-medium.onnx",
    "system_prompt": "You are a helpful robot assistant.",
    "chat_memory": true,
    "camera_rotation": 180
}
```

### Configuration Options

| Option | Default | Description |
| :--- | :--- | :--- |
| `text_model` | `gemma3:1b` | The LLM used for chatting. Must be installed via `ollama pull`. |
| `vision_model` | `moondream` | The model used when the agent "sees" something. |
| `voice_model` | `piper/en...` | Path to the `.onnx` voice model to use. |
| `chat_memory` | `true` | If `true`, remembers context. Set to `false` for a fresh start every time. |
| `camera_rotation` | `0` | Rotates the camera input (0, 90, 180, 270). |
| `system_prompt` | `...` | The core personality instructions for the AI. |

---

## ğŸ¨ Customizing Your Character

This software is just the brain. You define the character by adding assets:

1.  **Wake Word:** Train a custom model (e.g., "Hey Robot") using [OpenWakeWord](https://github.com/dscripka/openWakeWord) and save it as `wakeword.onnx`.
2.  **Faces:** The script looks for PNG sequences in `faces/[state]/`. It will loop through all images found in the folder to create an animation.
3.  **Sounds:** Put multiple `.wav` files in the `sounds/[category]/` folders. The robot will pick one at random each time (e.g., different "thinking" hums or "error" buzzes).

---

## âš ï¸ Troubleshooting

* **ğŸ”Š Audio Errors:** If you see ALSA errors in the terminal, don't worry. The script automatically detects your microphone's native sample rate and resamples audio on the fly to prevent crashes.
* **Web Search:** This agent uses `DuckDuckGo` to search the web. It prioritizes News headlines first, then falls back to general text search.
* **Security:** This script executes local commands (like `rpicam-still`). Do not modify the code to accept raw shell commands from the LLM.

## ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.
**Disclaimer:** This software is provided "as is", without warranty of any kind.
